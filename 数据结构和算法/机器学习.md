# 机器学习算法笔记

## 1. 决策树

1.   什么是决策树？

     决策树是基于树结构进行决策。非叶子节点对应一个属性的测试，树枝代表该属性的取值类别。叶子节点对应决策结果。

2.   决策树是如何对数据进行分类的？

     数据集从根节点出发，根据根据节点的属性以及属性的值，选择不同的分值。每经过一个节点分值，就会对数据集进行划分。直到所有的数据到达叶子节点，完成分类。

3.   如何利用样本集构建决策树？

     构建决策树中最重要的步骤是选择节点属性。即在当前的数据集中使用哪一个属性分类样本？

4.   常用的划分方法有哪些？

     -   使用信息熵和信息增益

         -   缺点：信息增益准则对可取值数据较多的属性有所偏好（例如属性A取5个值，另外属性B 取10个值，则属性B的信息增益大概率较大）。

     -   为了解决上述问题，可以使用增益率来选择最优划分属性(C4.5决策树算法)。增益率的定义为：

         $$Gain_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}$$，其中IV(a)的定义如下：

         $$IV(a) = -\sum_{v=1}^{V}{\frac{|D^v|}{|D|}\log_2{\frac{|D^v|}{|D|}}}$$

         增益准则对可选数据较少的属性有所偏好。

-   基本方法：信息熵和信息增益

    信息熵计算方式：当前样本集合D共有类别|y|种，第k种类别占的比例是$p_k (k = 1, 2, 3, ..., |y|)$，则集合D的信息熵是$$ Ent(D) = -\sum_{k=1}^{|y|} {p_k\log_2{p_k}} $$

    信息增益的计算方式：假设属性A有V个可能的值${a^1, a^2, ..., a^v}$，如果使用属性A对样本集进行划分，则会产生V个分支节点，其中第v个分支节点包含D中所有在属性a上取值为$a^v$的样本，记作$D^v$，同时给分支节点赋予权重$|D^v|/|D|$，则该属性获得的信息增益是：
    
    $$Gain(D, a) = Ent(D) -\sum_{v=1}^V{\frac{|D^v|}{|D|}Ent(D^v)}$$ 

​			信息增益越大，说明经过属性a的划分，数据集分类效果越好。

5.   什么是过拟合问题？

     训练样本太好，将训练集自身的特点当做所有数据项具有的一般性质。

5.   如何处理过拟合问题？

     可以通过剪枝处理过拟合。有两种剪枝策略：预剪枝和后剪枝。

     -   预剪枝：对每一个节点在划分前进行评估，如果划分后决策树的泛化性能没有提升，则停止该节点的划分（剪掉该节点的分枝）
     -   后剪枝：先从训练集生成一颗决策树，然后自低向上对非叶子节点进行考察，如果将该节点对应的子树替换为叶节点，能够提升泛化能力，则应该替换为叶节点。

6.   什么是泛化能力？如何判断泛化能力是否提升？

     使用验证集数据，验证该节点剪枝前后的准确率是否提升。如果准确率提升，则应该分支；准确率下降，则应该剪枝。

7.   预剪枝中如何判断泛化能力是否提升？

     -   使用验证集的数据将节点标记为具体的类别。
         -   使用验证集的数据计算在该节点中正确率。
     -   根据该节点的属性值，划分验证集，然后根据每一个子节点中的子验证集，标记子节点的类别。
         -   然后再计算子节点划分后，所有节点的正确率。
     -   判断正确率是否提升。

8.   预剪枝的优缺点

     -   优点：降低过拟合，减少决策树的训练时间开销。
     -   缺点：有些分支当前不能提升泛化，但是后续划分可能提升明显，全部剪掉容易带来欠拟合。

9.   后剪枝中如何判断泛化能力是否提升？

     -   先正常生成一颗决策树
     -   然后从底层叶子节点开始，寻找该叶子节点的父节点。如果训练集在父节点的准确率高于叶子节点的准确率，则应该剪枝，使得父节点变为子节点。

10.   后剪枝的优缺点

      -   后剪枝的欠拟合分险较小。
      -   需要再生成决策树后进行，并且从底层向上逐一考察，训练时间开销较大。

11.   如何处理连续属性变量？

      可以使得连续属性离散化。在C4.5决策树算法中，使用二分法对连续属性进行离散化处理。

      -   将连续值从小到大进行排序$\{a^1, a^2, a^3, ..., a^n\}$

      -   取所有$a^i$和$a^{i+1}$的中位数作为划分点，形成划分点集合

          $${T_a = \{{\frac{a^i + a^{i+1}}{2} | i <= i <= n-1}}\}$$

      -   计算每一个划分点的信息增益，然后选择信息增益最大的作为该属性的划分点。
      -   和其他属性的信息增益比较，选择最大的信息增益属性，作为当前的节点。

在处理连续属性变量时，先离散化属性值，选出信息增益最大的划分点作为该属性的划分点，然后再参与所有属性的比较。

12.   如何处理缺失值？
      -   如何在存在属性值缺失的情况下选择属性？
          -   使用无缺失样本计算属性的增益
          -   属性的增益再乘以无缺失样本的比例作为最后的信息增益
      -   样本在属性上有缺失，如何划分样本？
          -   缺失属性的样本进入每一个子节点
          -   缺失属性在每一个子节点中的权重不同，权重等于该属性值中无缺失属性样本数 / 总的无缺失属性样本数

## 2. 集成学习

### 2.1 Boosting 算法

### 2.2 Bagging算法

1.   bagging算法基于自助采样法进行。什么是自助采样算法？

​	给定包含m个样本的数据集，进行m次有放回的随机采样，可以得到包含m个样本的采样集。

2.   Bagging算法的流程是什么？
     -   使用自助采样法，取得T个采样数据集
     -   基于每一个采样数据集，训练一个学习器
     -   在预测最后的结果输出时，对分类任务使用投票输出法；对回归任务使用简单平均法。
3.   使用bagging算法时，大约有36.8%的数据不会被抽到，可以作为验证数据集。

### 2.3 随机森林算法

随机森林算法以决策树为基学习器构建bagging集成的基础上，在决策树训练过程中引入随机属性选择。在选择最佳属性时优化策略如下：

先从节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优的属性用于划分样本集。

## 3. XGBoost算法







